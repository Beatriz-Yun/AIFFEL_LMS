{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[E-04]자연어처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNalDnxyNC+QiN45m5iXONV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beatriz-Yun/AIFFEL_LMS/blob/main/Exploration/%5BE-04%5D%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRkwh4VK4chH",
        "outputId": "3d6acd94-64ce-45d7-ceca-f918a73e8cc2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XqepudsAj8"
      },
      "source": [
        "우리 주변의 모든 텍스트는 시퀀스 데이터이다.\n",
        "\n",
        "**시퀀스(sequence)**는 데이터에 순서(번호)를 붙여 나열한 것이다.\n",
        "- 특정 위치의 데이터를 가리킬 수 있다.\n",
        "- 정렬되어 있지 않아도 된다.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "문장을 구성하는 단어들은 문법이라는 규칙을 따라 나열되어 있는데,<br>\n",
        "이를 통해 텍스트 데이터를 예측하는 것은 예외가 너무 많다.\n",
        "\n",
        "\n",
        "따라서 텍스트데이터는 **통계를 기반으로 예측**한다.<br>\n",
        "ex) 'I' 다음에는 'am'이 오면 반이상 맞는다고 한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "즉, 인공지능이 텍스트를 이해하는 방식은 어떤 문법적인 원리가 아니라 **수많은 텍스트를 읽게 함으로써**<br>\n",
        "통계적으로 다음에 올 텍스트를 예측하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6iOdZGps_fx"
      },
      "source": [
        "**RNN(순환신경망)**\n",
        "- \\<start\\>토큰을 첫 입력으로 받아 생성이 끝나면 \\<end\\>토큰을 생성한다.\n",
        "- **생성한 토큰(단어)을 다시 입력으로 사용**한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw5n2jyCuzGT"
      },
      "source": [
        "**언어 모델(Language Model)**: n−1개의 단어 시퀀스 $w_1, \\cdots, w_{n-1}$가 주어졌을 때, $n$번째 단어 $w_n$으로 무엇이 올지를 예측하는 확률 모델\n",
        "- 어떤 텍스트도 언어모델의 학습 데이터가 될 수 있다.\n",
        "- 테스트할 때는 일정한 단어 시퀀스가 주어지면 다음 단어, 그 다음 단어를 계속해서 예측하며 텍스트를 생성한다.\n",
        "\n",
        "파라미터$\\theta$로 모델링하는 언어모델 표현: $P(w_1, \\cdots, w_{n-1};\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYgvWzn64sP9"
      },
      "source": [
        "## <연극대사 생성 모델 만들기>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KqdirE3JIL7"
      },
      "source": [
        "## '데이터 전처리' 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgoUrKaU4x-H"
      },
      "source": [
        "### 1. 데이터 살피기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKXnh-bEr78v",
        "outputId": "2bdd36a6-8d86-49ec-a5bb-d2c74a8a8155"
      },
      "source": [
        "import re \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 파일을 읽기모드로 열고\n",
        "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/AIFFEL_LMS/data/shakespeare.txt'\n",
        "with open(file_path, \"r\") as f:\n",
        "    raw_corpus = f.read().splitlines()\n",
        "\n",
        "# 앞에서부터 10라인 출력\n",
        "print(raw_corpus[:9])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETyxj1m85TM-"
      },
      "source": [
        "- 화자이름과 공백으로 된 데이터 제거하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMppXSxE5RkO",
        "outputId": "94d1a598-786e-46f6-eadc-f47d85961443"
      },
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인\n",
        "        \n",
        "    print(sentence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before we proceed any further, hear me speak.\n",
            "Speak, speak.\n",
            "You are all resolved rather to die than to famish?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgHY0mw5mGg"
      },
      "source": [
        "### 2. 토큰화(Tokenize): 문장을 일정한 기준으로 쪼갠다.\n",
        "- 공백을 기준으로 쪼개보자.\n",
        " - 문장 부호 양쪽에 공백 추가\n",
        " - 모두 소문자로 변환\n",
        " - 특수문자 모두 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nqGZN1J5-vC"
      },
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 문장부호 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자(공백포함 특수문자)를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                     # 1\n",
        "    #print(sentence)\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2 (정규표현식을 활용한다)\n",
        "    #print(sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
        "    #print(sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
        "    #print(sentence)\n",
        "    sentence = sentence.strip()                             # 5\n",
        "    #print(sentence)\n",
        "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
        "    return sentence\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1voSy44j-A3O",
        "outputId": "0e49ebe8-4778-488a-aa97-e1193b622c18"
      },
      "source": [
        "# 문장이 어떻게 필터링되는지 확인 (print문 주석처리 이전)\n",
        "sent = \"This @_is ;;;sample        sentence. ten-year-old?\"\n",
        "print('입력문장->',sent)\n",
        "print(preprocess_sentence(sent))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력문장-> This @_is ;;;sample        sentence. ten-year-old?\n",
            "this @_is ;;;sample        sentence. ten-year-old?\n",
            "this @_is ;;;sample        sentence .  ten-year-old ? \n",
            "this @_is ;;;sample sentence . ten-year-old ? \n",
            "this is sample sentence . ten year old ? \n",
            "this is sample sentence . ten year old ?\n",
            "<start> this is sample sentence . ten year old ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJM3IN4A48ij"
      },
      "source": [
        "자연어처리 분야에서 모델의 입력이 되는 문장을 소스 문장(**Source Sentence**),<br> 정답 역할을 하게 될 모델의 출력 문장을 타겟 문장(**Target Sentence**) 라고 관례적으로 부른다.\n",
        "\n",
        "즉, source sentence는 X_train이고, target sentence는 y_train이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-me4Kyl8di8",
        "outputId": "d7133d66-f78b-41c9-afd6-fe5f71e6d4f7"
      },
      "source": [
        "# 여기에 전처리된 문장을 모은다.\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뛴다. (공백 및 화자이름)\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 전처리함수에 문장을 입력 후 전처리된 문장을 corpus리스트에 추가.\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)   # (print문 주석처리 이후)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 전처리된 문장 10개 확인\n",
        "corpus[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> before we proceed any further , hear me speak . <end>',\n",
              " '<start> speak , speak . <end>',\n",
              " '<start> you are all resolved rather to die than to famish ? <end>',\n",
              " '<start> resolved . resolved . <end>',\n",
              " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
              " '<start> we know t , we know t . <end>',\n",
              " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
              " '<start> is t a verdict ? <end>',\n",
              " '<start> no more talking on t let it be done away , away ! <end>',\n",
              " '<start> one word , good citizens . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7W5HzrE-cmO"
      },
      "source": [
        "### 3. 벡터화(Vectorize): 텍스트데이터를 숫자로 변환\n",
        "- **tf.keras.preprocessing.text.Tokenizer** 패키지 사용 [(참고1, ](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) [참고2)](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
        " - 입력: 전처리된 텍스트\n",
        " - 출력: 단어 사전(vocabulary)\n",
        "- 벡터화된 데이터를 **텐서(tensor)**라고 한다.\n",
        " - tensorflow로 만든 모델의 입출력데이터는 모두 텐서로 변환되어 처리된다.\n",
        "\n",
        "**Tokenizer.fit_on_texts:** Updates internal vocabulary based on a list of texts.\n",
        "<br>\n",
        "**Tokenizer.texts_to_sequences:** Transforms each text in texts to a sequence of integers.<br><br>\n",
        "\n",
        "**pad_sequence의 padding옵션:** padding='post'인 경우, 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다. 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용한다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeYhlOfw_0yi",
        "outputId": "394a4c41-c3bf-43e2-c50a-14baabf475e4"
      },
      "source": [
        "# 텐서플로우의 Tokenizer와 pad_sequences를 사용하여 토큰화한다.\n",
        "\n",
        "def tokenize(corpus):\n",
        "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) \n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2  143   40 ...    0    0    0]\n",
            " [   2  110    4 ...    0    0    0]\n",
            " [   2   11   50 ...    0    0    0]\n",
            " ...\n",
            " [   2  149 4553 ...    0    0    0]\n",
            " [   2   34   71 ...    0    0    0]\n",
            " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0907850890>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7frvWyCWEfxK"
      },
      "source": [
        "**벡터화된 데이터**를 확인해보면 모두 정수로 이루어져 있다.<br>\n",
        "이 숫자는 **단어사전의 인덱스**이다.<br>\n",
        "(0은 패딩문자 \\<pad\\>이다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVAcb66PENEd",
        "outputId": "4aa68001-e4ad-4a57-d91f-82f036a84a56"
      },
      "source": [
        "print(tensor[:3, :10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2  143   40  933  140  591    4  124   24  110]\n",
            " [   2  110    4  110    5    3    0    0    0    0]\n",
            " [   2   11   50   43 1201  316    9  201   74    9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37k1s35NEsLu"
      },
      "source": [
        "단어사전 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PJNwjv8ErCL",
        "outputId": "82cef852-b675-4e31-fb3a-471281fe2b2b"
      },
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : .\n",
            "6 : the\n",
            "7 : and\n",
            "8 : i\n",
            "9 : to\n",
            "10 : of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPk0-UqHFojc"
      },
      "source": [
        "### 4. Source sentence와 Target sentence로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDcIOsALFIAu",
        "outputId": "a461a867-09a2-4d59-8231-f3b5eb6555d3"
      },
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]\n",
        "\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
            "   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdNbBsSFHJRS"
      },
      "source": [
        "#### 데이터셋 객체 생성\n",
        "지금까지는 model.fit(x_train, y_train, ...)형태로 ndarray를 생성하여 학습했다.<br>\n",
        "그러나 tensorflow를 활용할 때는 **tf.data.Dataset 객체**를 생성하는 방법을 흔히 사용한다.<br>\n",
        "tensorflow에서 사용할 경우 <u>데이터 입력 파이프라인을 통한 속도 개선 및 편의 기능을 제공</u>하므로 꼭 알아두자.<br>\n",
        "\n",
        "[중요](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
        "\n",
        "<br>\n",
        "\n",
        "데이터셋을 tensor형태로 생성했으므로 **from_tensor_slices()**메소드를 사용하여 **tf.data.Dataset객체**를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39jVYA-LIsF4",
        "outputId": "5cd90bfc-e50b-40a8-a6ab-5f586100be07"
      },
      "source": [
        "BUFFER_SIZE = len(src_input)   # source sentence 수\n",
        "BATCH_SIZE = 256               # 한 번 학습할 데이터 수\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "# VOCAB_SIZE: 벡터화한 단어 개수\n",
        "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)   # 데이터셋을 배치크기씩 나눈다.\n",
        "                                                           # drop_remainder=True는 배치크기보다 작은 그룹을 버린다.\n",
        "dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn680IBeizwR"
      },
      "source": [
        "## 모델 구성 및 학습과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUvpK4sgi7vp"
      },
      "source": [
        "### 모델의 구조도\n",
        "\n",
        "- tf.keras.Model을 Subclassing(상속)하는 방식으로 만들 것이다.\n",
        "[(Subclassing Model)](https://www.tensorflow.org/guide/keras/custom_layers_and_models?hl=ko)\n",
        "- 만들 모델은 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
        "\n",
        "![이미지](https://drive.google.com/uc?id=1sjQXo2D9b5p83ypaHjmm6ZU967H4hpA2)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Embedding레이어:** 단어사전(voocabulary)에 있는 인덱스 값을 <u>해당 인덱스번째의 word vector로 바꿔준다.</u>\n",
        "- **word vector**는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsuGc0gEjAzx"
      },
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "  # embedding_size는 word vector의 차원수이다.\n",
        "  # hidden_size는 LSTM레이어의 hidden state의 차원수이다.\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbruydB7m2IL"
      },
      "source": [
        "모델의 최종 출력 텐서의 shape이 (256, 20, 7001)임을 확인할 수 있다.\n",
        "- 7001은 Dense레이어의 출력 차원수이다.\n",
        " - 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문이다.\n",
        "- 256은 이전단계에서 지정한 배치 사이즈이다.\n",
        " - dataset.take(1)은 데이터셋에서 1개의 배치를 가져온다.\n",
        "- 20은 LSTM레이어에서 **return_sequences=True**로 지정함으로써 나타난다.\n",
        " - LSTM이 **자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력**한다는 의미이다.\n",
        " - 즉, 우리의 데이터셋의 max_len이 20이었기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OycA1KWImowU",
        "outputId": "85bd2254-82db-4d50-a6c9-eacdd831cab0"
      },
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
              "array([[[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 7.18360941e-04, -4.11577130e-05,  1.03884209e-04, ...,\n",
              "         -1.40163596e-04, -6.53263938e-04, -4.31908382e-04],\n",
              "        [ 6.15979428e-04,  1.01792750e-04, -1.49642263e-04, ...,\n",
              "         -1.00366102e-04, -1.05997943e-03, -6.57598954e-04],\n",
              "        ...,\n",
              "        [ 3.78154218e-04, -2.25151097e-03, -2.55316473e-03, ...,\n",
              "         -3.29263159e-03, -1.43250008e-03,  1.30326510e-03],\n",
              "        [ 5.38947061e-04, -2.20816443e-03, -2.83878180e-03, ...,\n",
              "         -3.48402094e-03, -1.51188811e-03,  1.55652477e-03],\n",
              "        [ 6.96565898e-04, -2.13662582e-03, -3.11367470e-03, ...,\n",
              "         -3.64306709e-03, -1.58119528e-03,  1.75599975e-03]],\n",
              "\n",
              "       [[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 1.29127075e-04,  2.69704266e-04,  5.45959745e-04, ...,\n",
              "          1.93901447e-04, -4.86295350e-04, -4.36051749e-04],\n",
              "        [-2.88750780e-05,  4.25745384e-04,  4.38197050e-04, ...,\n",
              "          6.12733056e-05, -1.27901483e-04, -4.26213373e-04],\n",
              "        ...,\n",
              "        [ 4.45981656e-04, -1.78188516e-03, -2.25857226e-03, ...,\n",
              "         -6.44062879e-04, -9.33157513e-04,  9.58706310e-04],\n",
              "        [ 5.35940344e-04, -1.87867321e-03, -2.51553138e-03, ...,\n",
              "         -1.23789057e-03, -1.03767554e-03,  1.27780822e-03],\n",
              "        [ 6.43647742e-04, -1.92105770e-03, -2.75634159e-03, ...,\n",
              "         -1.76998379e-03, -1.12449937e-03,  1.55523058e-03]],\n",
              "\n",
              "       [[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 3.47445486e-04,  7.08043881e-05,  2.64821254e-04, ...,\n",
              "          1.60582626e-04, -7.10141729e-04, -3.20252671e-04],\n",
              "        [ 3.17813130e-04,  1.22028745e-04,  2.15048669e-04, ...,\n",
              "          4.28559069e-05, -9.10759438e-04, -5.66133007e-04],\n",
              "        ...,\n",
              "        [ 2.37654720e-04, -1.83426810e-03, -3.04344250e-03, ...,\n",
              "         -3.45423119e-03, -1.70550798e-03,  1.93508773e-03],\n",
              "        [ 4.78232192e-04, -1.77169743e-03, -3.30240815e-03, ...,\n",
              "         -3.63181625e-03, -1.74206775e-03,  2.08278815e-03],\n",
              "        [ 6.90153509e-04, -1.70482707e-03, -3.54430219e-03, ...,\n",
              "         -3.77682340e-03, -1.77857303e-03,  2.18999339e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 1.70162875e-05, -1.77261798e-04,  6.01923501e-04, ...,\n",
              "          1.11404996e-04, -2.20487695e-04, -8.20190180e-05],\n",
              "        [-8.96939964e-05, -3.75141797e-04,  7.66968238e-04, ...,\n",
              "          2.15623404e-05,  8.47493175e-06, -2.99275212e-04],\n",
              "        ...,\n",
              "        [ 8.28759628e-04, -1.40127353e-03, -1.67214451e-03, ...,\n",
              "         -8.56913102e-04, -1.46684819e-03, -5.26652380e-04],\n",
              "        [ 7.11952744e-04, -1.64428598e-03, -1.97346532e-03, ...,\n",
              "         -1.32998952e-03, -1.52033917e-03,  1.74425440e-05],\n",
              "        [ 6.56367920e-04, -1.81015371e-03, -2.26056646e-03, ...,\n",
              "         -1.78481673e-03, -1.55758380e-03,  5.11592661e-04]],\n",
              "\n",
              "       [[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 6.71553207e-05,  8.42019072e-05,  4.97627596e-04, ...,\n",
              "          3.08752438e-04, -3.81663529e-04,  4.39548603e-05],\n",
              "        [-2.69263110e-04, -3.98322241e-04,  1.02323880e-04, ...,\n",
              "          5.88645053e-04, -5.00607654e-04, -1.46405694e-06],\n",
              "        ...,\n",
              "        [ 3.13892873e-04, -2.80831871e-03, -2.54615513e-03, ...,\n",
              "         -2.81976443e-03, -1.94445299e-03,  2.42348667e-03],\n",
              "        [ 5.07246470e-04, -2.71101715e-03, -2.80408631e-03, ...,\n",
              "         -3.13793868e-03, -1.94302236e-03,  2.53949151e-03],\n",
              "        [ 6.92032161e-04, -2.58712331e-03, -3.05190263e-03, ...,\n",
              "         -3.40185314e-03, -1.94501225e-03,  2.60933954e-03]],\n",
              "\n",
              "       [[ 1.60345386e-04,  6.09239687e-05,  2.89983233e-04, ...,\n",
              "          1.42004836e-04, -2.85116927e-04, -1.33140813e-04],\n",
              "        [ 6.71553207e-05,  8.42019072e-05,  4.97627596e-04, ...,\n",
              "          3.08752438e-04, -3.81663529e-04,  4.39548603e-05],\n",
              "        [ 1.64783396e-05, -2.87786770e-05,  6.38180587e-04, ...,\n",
              "          3.24767199e-04, -2.35524305e-04, -1.89814615e-04],\n",
              "        ...,\n",
              "        [ 4.07355983e-04, -2.65646633e-03, -3.44926096e-03, ...,\n",
              "         -3.00651556e-03, -1.52637064e-03,  1.82239828e-03],\n",
              "        [ 6.05666486e-04, -2.54845293e-03, -3.66810942e-03, ...,\n",
              "         -3.30758980e-03, -1.56200840e-03,  1.98191474e-03],\n",
              "        [ 7.83398980e-04, -2.42425059e-03, -3.86726251e-03, ...,\n",
              "         -3.55179724e-03, -1.60198845e-03,  2.10218760e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhIk32VXoP3Z"
      },
      "source": [
        "우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape을 특정할 수 없다.<br>\n",
        "따라서 model.summary()를 해도 Ooutput Shape을 정확하게 알려주지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFZUx5aToHHs",
        "outputId": "1686b2f7-0e50-430c-d0fe-716857bb8b9a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  1792256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  7176025   \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcuB-gZipZSW"
      },
      "source": [
        "### 모델 학습시키기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMRjyZLropOA"
      },
      "source": [
        "**tf.test.is_gpu_available()**를 통해 tensorflow가 GPU를 사용하고 있는지 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf-IfJZlonau",
        "outputId": "39925f98-f0ee-483c-f655-817d8ce652aa"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLyd-42pjlB"
      },
      "source": [
        "\n",
        "[참고1. optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "\n",
        "\n",
        "[참고2. loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxGj0BVpUMD",
        "outputId": "911e03ae-9886-40cc-87d1-8d0b1315985d"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=30)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "93/93 [==============================] - 20s 180ms/step - loss: 3.4779\n",
            "Epoch 2/30\n",
            "93/93 [==============================] - 17s 182ms/step - loss: 2.8068\n",
            "Epoch 3/30\n",
            "93/93 [==============================] - 17s 184ms/step - loss: 2.7089\n",
            "Epoch 4/30\n",
            "93/93 [==============================] - 18s 189ms/step - loss: 2.6105\n",
            "Epoch 5/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.5429\n",
            "Epoch 6/30\n",
            "93/93 [==============================] - 18s 193ms/step - loss: 2.4906\n",
            "Epoch 7/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.4369\n",
            "Epoch 8/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 2.3875\n",
            "Epoch 9/30\n",
            "93/93 [==============================] - 18s 190ms/step - loss: 2.3420\n",
            "Epoch 10/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 2.2941\n",
            "Epoch 11/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.2499\n",
            "Epoch 12/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.2052\n",
            "Epoch 13/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 2.1609\n",
            "Epoch 14/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 2.1183\n",
            "Epoch 15/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.0756\n",
            "Epoch 16/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 2.0330\n",
            "Epoch 17/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.9900\n",
            "Epoch 18/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.9472\n",
            "Epoch 19/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.9044\n",
            "Epoch 20/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.8613\n",
            "Epoch 21/30\n",
            "93/93 [==============================] - 18s 190ms/step - loss: 1.8187\n",
            "Epoch 22/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.7750\n",
            "Epoch 23/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.7320\n",
            "Epoch 24/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.6895\n",
            "Epoch 25/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.6487\n",
            "Epoch 26/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.6061\n",
            "Epoch 27/30\n",
            "93/93 [==============================] - 18s 191ms/step - loss: 1.5626\n",
            "Epoch 28/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.5220\n",
            "Epoch 29/30\n",
            "93/93 [==============================] - 18s 192ms/step - loss: 1.4805\n",
            "Epoch 30/30\n",
            "93/93 [==============================] - 18s 190ms/step - loss: 1.4381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0907b22850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39fGodb6qBnA"
      },
      "source": [
        "## 모델 평가\n",
        "\n",
        "모델이 작문을 잘하는지 컴퓨터 알고리즘이 평가하는 것은 무리가 있다. 그래서 작문을 시켜보고 직접 평가해야 한다.\n",
        "\n",
        "시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 하는 함수generate_text를 작성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SzH1NAuqDkC"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCWD0xuptKVW"
      },
      "source": [
        "텍스트를 생성해야 하는데 지금 source sentence와 target sentence가 없다.\n",
        "\n",
        "이전에 테스트 데이터셋을 생성하지도 않았다.\n",
        "\n",
        "init_sentence를 \\<start\\>로 시작하는 텍스트로 줘서 텍스트생성함수를 여러번 실행해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YVyP64ZDtFiy",
        "outputId": "df496578-a174-4725-ee30-29afd527c9df"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> he hath been a suitor to the people , but that <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gds5vnMatwgt",
        "outputId": "0b5b1a2b-cf53-4108-cf25-e3c746cdaf57"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> you have made a man of a <unk> , <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YPwnrH88t1Wc",
        "outputId": "68370af8-4611-48c8-9509-aebe9dffa973"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> are\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> are you so hot ? <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}